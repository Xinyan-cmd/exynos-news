name: Strict Content Indexing
on:
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC

jobs:
  strict-indexer:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Dependencies
        run: |
          sudo apt-get -qq update
          sudo apt-get -qq install git jq dos2unix
          pip3 -qq install beautifulsoup4 python-dateutil

      - name: Normalize Line Endings
        run: |
          find docs/news/current -name "*.html" -exec dos2unix {} \;

      - name: Validate Posts
        run: |
          cat << 'EOF' > validate_posts.py
          import sys
          import os
          import subprocess
          from bs4 import BeautifulSoup
          from datetime import datetime

          POSTS_DIR = 'docs/news/current'
          DATE_FORMAT = '%Y-%m-%d'

          def get_git_date(filepath):
              result = subprocess.run(
                  ['git', 'log', '--diff-filter=A', '--format=%ad', '--date=short', '--', filepath],
                  capture_output=True, text=True
              )
              dates = result.stdout.strip().split('\n')
              return dates[-1] if dates else None

          errors = []
          for filename in os.listdir(POSTS_DIR):
              if filename == 'index.html' or not filename.endswith('.html'):
                  continue

              path = os.path.join(POSTS_DIR, filename)
              with open(path) as f:
                  soup = BeautifulSoup(f, 'html.parser')

              # Date detection
              meta_date = soup.find('meta', {'name': 'date'}) or \
                          soup.find('meta', {'property': 'article:published_time'})
              visible_date = soup.find(lambda tag: tag.name == "p" and "Published:" in tag.text)
              git_date = get_git_date(path)

              try:
                  date_str = None
                  if meta_date and visible_date:
                      errors.append(f"‚ö†Ô∏è {filename}: Both meta and visible dates present")
                  elif meta_date:
                      date_str = meta_date.get('content', '').split('T')[0].strip()
                  elif visible_date:
                      date_str = visible_date.text.split('Published:')[-1].strip().split('_')[0].strip()
                  else:
                      raise ValueError("No valid date source found")

                  # Final validation
                  datetime.strptime(date_str, DATE_FORMAT)
                  
              except Exception as e:
                  errors.append(f"üö´ {filename}: {str(e)}. Using git date: {git_date}")

          if errors:
              print("\nValidation Report:")
              for error in errors:
                  print(error)
          
          sys.exit(0)
          EOF
          python3 validate_posts.py

      - name: Generate Index
        run: |
          cat << 'EOF' > generate_index.py
          import os
          import json
          import subprocess
          from bs4 import BeautifulSoup
          from datetime import datetime

          def get_git_date(filepath):
              result = subprocess.run(
                  ['git', 'log', '--diff-filter=A', '--format=%ad', '--date=short', '--', filepath],
                  capture_output=True, text=True
              )
              dates = result.stdout.strip().split('\n')
              return dates[-1] if dates else None

          index = {
              "meta": {
                  "schema": "2.2-strict",
                  "generated": datetime.utcnow().isoformat(),
              },
              "posts": [],
              "warnings": []
          }

          for filename in os.listdir('docs/news/current'):
              if filename == 'index.html' or not filename.endswith('.html'):
                  continue

              path = os.path.join('docs/news/current', filename)
              with open(path) as f:
                  soup = BeautifulSoup(f, 'html.parser')

              # Date extraction
              meta = soup.find('meta', {'name': 'date'}) or \
                     soup.find('meta', {'property': 'article:published_time'})
              visible = soup.find(lambda tag: tag.name == "p" and "Published:" in tag.text)
              git_date = get_git_date(path)

              date = git_date
              source = 'git'
              validation = 'valid'

              try:
                  if meta:
                      date = meta.get('content', '').split('T')[0].strip()
                      source = 'meta'
                  elif visible:
                      date = visible.text.split('Published:')[-1].strip().split('_')[0].strip()
                      source = 'visible'
                      
                  datetime.strptime(date, '%Y-%m-%d')
              except ValueError:
                  validation = 'invalid'
                  index['warnings'].append(f"{filename}: Invalid date format, using git date {git_date}")
              except Exception as e:
                  validation = 'error'
                  index['warnings'].append(f"{filename}: {str(e)}")

              index['posts'].append({
                  "path": filename,
                  "title": soup.title.text.replace('| Exynos-News', '').strip(),
                  "date": date,
                  "source": source,
                  "validation": validation
              })

          index['posts'].sort(key=lambda x: x['date'], reverse=True)
          
          with open('docs/news/_index.json', 'w') as f:
              json.dump(index, f, indent=2)
          EOF
          python3 generate_index.py

      - name: Commit Changes
        run: |
          git config --global user.name "Index Guardian"
          git config --global user.email "guardian@exynos-news"
          git add docs/news/_index.json
          git commit -m "üîí Index update: $(date +'%Y-%m-%d')" || echo "No changes detected"
          git push

      - name: Final Report
        run: |
          echo "### Indexing Report" >> $GITHUB_STEP_SUMMARY
          echo '```json' >> $GITHUB_STEP_SUMMARY
          jq '{post_count: .posts | length, valid_posts: [.posts[] | select(.validation == "valid")] | length}' docs/news/_index.json >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "#### Warnings" >> $GITHUB_STEP_SUMMARY
          jq -r '.warnings[]' docs/news/_index.json >> $GITHUB_STEP_SUMMARY
