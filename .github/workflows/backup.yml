name: Nuclear Indexing System
on:
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC

jobs:
  nuclear:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Purge Previous Artifacts
        run: |
          # Delete all Python scripts and old index
          find . -name "*.py" -delete
          rm -f docs/news/_index.json
          git add -A
          git commit -m "ðŸ§¹ Nuclear cleanup" || echo "Nothing to clean"

      - name: Generate Fresh Index
        run: |
          cat << 'EOF' > nuclear_index.py
          import os
          import json
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup

          INDEX_PATH = 'docs/news/_index.json'
          POSTS_DIR = 'docs/news/current'

          def extract_content_date(soup):
              try:
                  # First try meta tags
                  meta_date = soup.find('meta', {'name': 'date'}) or \
                             soup.find('meta', {'property': 'article:published_time'})
                  if meta_date:
                      return meta_date['content'].split('T')[0]
                      
                  # Then look for visible date
                  header = soup.find('header', class_='post-header')
                  if header:
                      date_tag = header.find('p', string=lambda t: 'Published:' in t)
                      if date_tag:
                          return date_tag.get_text().split('Published:')[-1].strip(' >_')
              except:
                  pass
              return None

          index = {
              "meta": {
                  "generated": datetime.now(timezone.utc).isoformat(),
                  "schema": "nuclear-5.0"
              },
              "posts": []
          }

          for filename in os.listdir(POSTS_DIR):
              if filename == 'index.html' or not filename.endswith('.html'):
                  continue
              
              path = os.path.join(POSTS_DIR, filename)
              with open(path) as f:
                  soup = BeautifulSoup(f, 'html.parser')
                  
              content_date = extract_content_date(soup)
              title = soup.title.text.replace('| Exynos-News', '').strip() if soup.title else filename
              
              index['posts'].append({
                  "path": filename,
                  "title": title,
                  "date": content_date or "1970-01-01",
                  "valid": bool(content_date)
              })

          index['posts'].sort(key=lambda x: x['date'], reverse=True)
          
          with open(INDEX_PATH, 'w') as f:
              json.dump(index, f, indent=2)
          EOF

          python3 nuclear_index.py
          rm nuclear_index.py  # Self-clean

      - name: Commit New Reality
        run: |
          git config --global user.name "Nuclear Overseer"
          git config --global user.email "nuke@exynos-news"
          git add .
          git commit -m "ðŸ’¥ Nuclear index rebuild: $(date +'%Y-%m-%d %H:%M')"
          git push --force

      - name: Post-Apocalyptic Report
        run: |
          echo "### Nuclear Report" >> $GITHUB_STEP_SUMMARY
          echo "Index Version: $(jq -r '.meta.schema' docs/news/_index.json)" >> $GITHUB_STEP_SUMMARY
          echo "Valid Posts: $(jq '[.posts[] | select(.valid)] | length' docs/news/_index.json)" >> $GITHUB_STEP_SUMMARY
